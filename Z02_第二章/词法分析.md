## 直接整数与词法分析

让我们先休整一下，做个更简单的用例：

```
43
```

这段Z代码实际上比`print("hello")`还要简单，它才是真正的“最简单的Z程序”。

但是考虑到Hello World无与伦比的影响力，我还是先写了Hello World，再写这一节。

那么这时候引入这更简单的例子，是在走回头路吗？
并不是。引入了这个用例之后，Z语言实际上扩展了。

之前我们只有一种形式：`print(...)`，而现在有了两种形式：`print(...)`和`43`。

前者是一个函数调用的表达式，后者是一个整数表达式。我们的编译器需要区分这两种情况。

那么，如何区分`print`和`43`呢？这就需要引入词法的概念了。

在编译器技术中，把源代码分解成一个个不同类型的“词”，被称为“词法分析”（Lexical Analysis）。

这种词和我们平日所说的词（word）不同，它并不是一个个有具体含义的词语，而是把源码切分之后的最小单元。
为了和平日的词（word）区分，这种词被称为Token。

例如，`print`是一个Token，表达的是一个函数的名称；`43`也是一个Token，表达的是一个整数。
而`(`和`)`也是Token，分别表达函数参数列表的开始与结束。
`"Hello"`是一个Token，表达的是一个字符串。注意，字符串前后的引号也是它的一部分。

所以，词法分析的结果，就是把源码切分成一个个Token。

以`print("hello")`为例，词法分析的结果如下：

```
<NAME:print>
<LPAREN:(>
<STR:"hello">
<RPAREN:)>
```

其中，`<NAME:print>`这个Token，它的类型是“函数名称”，我们称之为`NAME`（教科书里称为IDENT，即Identifier，标识符）

Token这个词一般翻译为“令牌”，但我认为这是“令牌”是Token这个多义词在另一个语境里（网络安全）所表达的意思，在语言学里，并不没有“令牌”的含义。
所以我把它叫做“词符”，以和更基本的单元“字符”（诸如a,b,c之类）区分。

本书后面的章节里，提到“词符”，就是代指Token的意思。

分析出词符之后，我们就可以方便地区别源码的结构了：

例如，要区分`print(43)`和`43`，只需要看第一个词符的类型就可以了：

- 如果第一个词符是`NAME`，那么就是函数调用表达式。
- 如果第一个词符是`INT`，那么就是整数表达式。

注意，这是在当前Z语言只有这两种表达式的前提下，判断才如此简单。未来添加了更多的语句结构之后，这种判断就比较复杂了。

根据词符的序列，判断语句的结构，并组装成一棵AST语法树的过程，就是“语法分析”（Syntactic Analysis）。我们一般叫`Parser`。
而从源码解析为词符序列的过程，就是“词法分析”（Lexical Analysis）。我们一般叫`Lexer`（也有的编译器叫`Scanner`）。

本书沿用`Lexer`和`Parser`的概念。接下来我们实现`Lexer`，即词法分析器。

## 测试配置

在编写词法分析器`Lexer`之前，我们需要先添加整数表达式的测试用例。

而在添加测试用例之前，我发现还可以整理并简化一下测试的配置。

上一节的测试中我发现，把不同的用例放在不同的目录之后，假定目录名称是`dir`，其实可以采用统一的名称：

- `dir_case.z`：Z语言源码，即测试输入用例。
- `dir_expected.*`：期望的输出文件。
- `app.*`：实际的输出文件。

有了这样统一的命名，`xmake.lua`中的用例配置就会更加简单了。
只需要遍历不同用例的目录，就可以用一套配置覆盖所有用例。

```lua
local case_list = {"hello", "simple_int"}

-- 编译器compiler的测试用例
target("test_compiler")
    set_kind("binary")
    set_default(false)
    add_includedirs("src")
    add_files("src/*.c")
    remove_files("src/main.c")
    add_files("test/test_compiler.c")
    for _, d in ipairs(case_list) do
        local asm_ext = is_plat("windows") and "asm" or "s"
        add_tests(d, {rundir = os.projectdir().."/test/"..d, runargs = {d.."_case.z", d.."_expected."..asm_ext}})
    end

-- 转译器transpiler的测试用例
target("test_transpiler")
    set_kind("binary")
    set_default(false)
    add_includedirs("src")
    add_files("src/*.c")
    remove_files("src/main.c")
    add_files("test/test_transpiler.c")

    for _, d in ipairs(case_list) do
        for _, lan in ipairs({"c", "py", "js"}) do
            add_tests(d.."_"..lan, {rundir = os.projectdir().."/test/"..d, runargs = {lan, d.."_case.z", d.."_expected."..lan}})
        end
    end
```

以后我们添加新的测试用例时，只需要修改`case_list`就可以了。


## 测试用例

下面我们添加整数表达式`43`的测试用例，取名为`single_int`。

#### 解释器

最简单的用例是解释器：

```lua
add_tests("single_int", {runargs="42", trim_output=true, pass_outputs="42"})
```

解释器输入是`42`，输出也应当是`42`，这是最简单的用例。

然后为编译器和转译器提供Z语言文件，`simple_int_case.z`：

```z
42
```

#### 编译器

对于汇编和C的输出，我们期望的是函数直接返回`42`，而不用打印，所以期望的汇编文件会简单很多：

```asm
includelib msvcrt.lib
.code
main proc
    mov rax, 43
    ret
main endp
end
```

直接将寄存器`rax`赋值为`43`，然后调用`ret`返回即可。

甚至可以更进一步，如果不考虑引入C语言标准库，可以写一个裸汇编，把整个程序的入口直接指定为`main`：

```asm
.code
main proc
    mov rax, 43
    ret
main endp
end
```

这个汇编在编译时需要制定入口函数：

```bash
$ ml64 simple_int_case.asm /link /entry:main
```

这就是理论上最简单的windows汇编程序了。但考虑到我们之后必然要引入C标准库，所以还是先用第一个版本。

这个用例放在`single_int_expected.asm`文件中。

Linux汇编如下：

```asm
    .intel_syntax noprefix
    .text
    .global main
main:
    mov rax, 42
    ret
```

由于`main`函数直接返回，没有对栈空间进行任何操作，所以连常见的`prologue`和`epilogue`都可以省略了。
这也是理论上最简单的Linux汇编程序了。

把这个文件存到`single_int_expected.s`中。

#### 转译器

对于转译器，我们期望的C语言输出是：

```c
#include <stdio.h>
int main(void) {
    return 42;
}
```

而Python和JavaScript的期望输出则非常简单：

```python
42
```

```javascript
42
```

这两个语言都是动态解释语言，因此可以直接将整数解释出来。

现在所有的用例都准备好了，开始编写代码。

## 词符和词法分析器的定义

我们先将原始的`parse_expr`拆分成`lex`和`parse`两个模块。

`lex`的功能是读取源码，解析成一个个词符；
而`parse`的功能是读取一个个词符，对其进行判断，并组装成AST语法树。

我们先定义词符的概念：

```c
// 词符的种类
typedef enum {
    TK_NAME, // 名称
    TK_INT, // 整数
    TK_LPAREN, // (
    TK_RPAREN, // )
    TK_STR, // 字符串
    TK_EOF, // 源码结束
} TokenKind;

// 词符
struct Token {
    TokenKind kind;
    const char *pos; // 指向词符在源码中的起始位置
    size_t len; // 词符的长度
};
```

每个词符记录的信息有：

- 种类
- 源码中的起始位置
- 长度

种类是在词法分析时，就可以通过下一个字符以及当前的状态来判断。
具体参考词法分析的实现。

而起始位置和长度可以用来从源码中获取词符的内容。

然后我们定义词法分析器`Lexer`：

```c
// 词法分析器
struct Lexer {
    char* start; // 解析的起始位置。每解析完一个词符，start就会被更新。
    char* cur; // 解析的当前位置。解析完一个词符时，start到cur之间的字符串就是词符的内容。
};


// 新建一个词法分析器
Lexer *new_lexer(const char *code);

// 解析下一个词符
Token *next_token(Lexer *lexer);
```

现在的`Lexer`很简单，只需要记录解析的起始位置`start`和解析过程中的当前位置`cur`即可。
每解析出一个新的词符，`cur`就会走到这个词符的末尾。要进行下一个词符之前，我们需要把`start`挪到`cur`的位置，从这里开始。
这样的话，`start`到`cur`之间的字符串，就是一个词符的内容。
也就是说对每个词符来说：`token.pos == lexer.start`，`token.len == lexer.cur - lexer.start`。

## 词法分析的实现

`new_lexer`的很简单，只要把`start`和`cur`都指向源码的起始位置即可。

```c
Lexer *new_lexer(const char *code) {
    Lexer *lexer = calloc(1, sizeof(Lexer));
    lexer->start = code;
    lexer->cur = code;
    return lexer;
}
```

`next_token`是实际执行词法解析的函数。它就复杂多了。

```c
// 解析下一个Token
Token *next_token(Lexer *lexer) {
    // 更新start指针，指向上个Token的末尾
    lexer->start = lexer->cur;

    // 如果遇到文件或源码末尾，就返回TK_EOF
    if (is_eof(lexer)) {
        return new_token(lexer, TK_EOF);
    }
    // 读取一个字符
    char c = next_char(lexer);

    // 如果是数字
    if (is_digit(c)) {
        return number(lexer);
    }

    // '"'代表字符串
    if (c == '"') {
        return str(lexer);
    }

    // 如果是字母或下划线
    if (is_alpha(c)) {
        return name(lexer);
    }

    // 处理其他的特殊字符
    switch (c) {
    case '(':
        return new_token(lexer, TK_LPAREN);
    case ')':
        return new_token(lexer, TK_RPAREN);
    }
}
```

可以看出来，`next_token`的一个个字符读取并判断：

- 是否到了文件末尾，如果是，返回`TK_EOF`。
- 字符是数字的话，就调用`number`函数解析出一个整数。
例如，遇到'1234'，根据'1'判断出是数字，然后`number`函数会继续读取'2'、'3'、'4'，直到遇到非数字字符为止，结果就是`1234`。
- 是`"`的话，则表示遇到了一个字符串，需要调用`str`来解析出字符串。
逻辑也不复杂，一直往前读取，直到遇到另一个`"`为止。
- 如果是字母或下划线（即`is_alpha`）的话，那就是某种名称，例如函数名称、类型名称、变量名称等。
我们统一称之为`NAME`。调用`name`函数解析出名称。名称的规则是首字母满足`is_alpha`，后面的字符可以是`is_alpha`或`is_digit`。
- 这几类词符都判断过了，剩下的可能性就只有'('或')'了，这两个是特殊的分隔符，分别返回对应的词符。

`next_token`调用到的辅助函数：

- `is_digit`、`is_alpha`、`is_alnum`、`is_eof`都是简单的字符判断函数。
- `number`、`str`、`name`三个解析函数，会从当前位置继续读取字符，直到凑齐一个词符为止。
- `next_char`和`new_token`都是辅助用的函数，看字面意思即可。

这样的一个词法解析器，可以支持名称、整数、字符串和括号这四类词符的任意组合。
但如果代码中有其他不属于这四类的字符，例如`#`，就没法处理了。
下一节，我们会加上报错逻辑，这样的话遇到不认识的字符，就会报出合理的提示信息了。

注意：词法解释器并不管各种词符出现的顺序是否合理，那是后面语法解析器的工作。

例如，下面几行代码，对词法解析器来说都是可以解读的：

- `print(1)`。这是正常的代码，解析出的词符是`NAME`、`LPAREN`、`INT`、`RPAREN`。
- `25)haha(8`。这显然是错误的代码，但词法解析器也会解析出来：`INT`、`RPAREN`、`NAME`、`LPAREN`、`INT`。

后续的语法解析器需要判断在某种情况下，哪些词符是合法的，出现非法词符的时候应当报出语法错误。

## 整合词法和语法解析器

现在有了一个能用的`Lexer`了，我们需要修改`parse_expr`，让它使用`Lexer`来解析源码。

我们先把之前的`parse_expr`的逻辑挪出去，变成一个`parse_call`函数。

在`parse_expr`中，先新建一个`Lexer`，然后调用`next_token`来获取第一个词符。

根据这个词符，我么就可以判断这行代码是一个函数调用表达式，还是一个整数表达式。

```c
// 先新建一个词法分析器
Lexer *lexer = new_lexer(code);
// 获得第一个词符
Token *token = next_token(lexer);

if (token->kind == TK_INT) {
    // 如果是整数
    return parse_int(lexer, code);
} else {
    // 否则就是一个函数调用
    return parse_call(lexer, code);
}
```

`parse_call`和之前的`parse_expr`一样；
`parse_int`负责解析一个整数表达式。

整数表达式非常简单：

```c
static Node *parse_int(Lexer *lexer, char *code) {
    Node *expr = calloc(1, sizeof(Node));
    expr->kind = ND_INT;
    char *num_text = substr(code, lexer->start - code, lexer->cur - code);
    expr->as.num = atod(num_text);
    trace_node(expr);
    return expr;
}
```

直接用`substr`获取到词符的内容（即整数的字符串表达形式），再调用`atod`函数将字符串解析成`int`类型的整数即可。
把这个整数值填入到一个`ND_INT`类型的`Node`中，返回即可。

现在可以运行`xmake test`进行测试了，遇到`fail`的情况是正常的，根据具体情况调试修改代码即可。

我遇到了两个问题，一个是函数实现有点小错，通过debug找到了。另一个是测试用例的文件写错了（毕竟是手写的），修改用例文件即可。

现在所有的测试，包括新增的`single_int`用例，都通过了！

```bash
$ xmake test
running tests ...
[  6%]: test_compiler/hello           .................................... passed 0.000s
[ 12%]: test_compiler/simple_int      .................................... passed 0.016s
[ 18%]: test_compiler/single_int      .................................... passed 0.000s
[ 25%]: test_interp/hello             .................................... passed 0.016s
[ 31%]: test_interp/hello1            .................................... passed 0.015s
[ 37%]: test_interp/simple_int        .................................... passed 0.000s
[ 43%]: test_interp/single_int        .................................... passed 0.000s
[ 50%]: test_transpiler/hello_c       .................................... passed 0.016s
[ 56%]: test_transpiler/hello_js      .................................... passed 0.000s
[ 62%]: test_transpiler/hello_py      .................................... passed 0.016s
[ 68%]: test_transpiler/simple_int_c  .................................... passed 0.000s
[ 75%]: test_transpiler/simple_int_js .................................... passed 0.015s
[ 81%]: test_transpiler/simple_int_py .................................... passed 0.000s
[ 87%]: test_transpiler/single_int_c  .................................... passed 0.016s
[ 93%]: test_transpiler/single_int_js .................................... passed 0.015s
[100%]: test_transpiler/single_int_py .................................... passed 0.000s

100% tests passed, 0 tests failed out of 16, spent 0.125s
```

鼓掌！

现在我们可以打开`repl`试一下了：

```bash
$ xmake run z repl
Z REPL v0.1
--------------
>>> print(1)
1
--------------
>>> print("Hello")
Hello
--------------
>>> 33
33
--------------
>>>
```

你看，现在`print`和`33`这样的单个整数都可以跑了！

又完成了一个小小的里程碑。我们提交一个新标签：

```bash
$ git commit -a -m "步骤11：整数与词法解析"
$ git tag v0.0.11 -m "步骤11：整数与词法解析"
$ git push
```

## 小结

本节做了三件事情：

- 重构了测试的配置，以后添加新的测试用例更加简单了
- 设计了单个整数的测试用例
- 简单描述了词法分析和词符
- 实现了针对当前Z语法的词法分析器

现在的Z语言，已经不像上一章那么单调了，但是还不够动感。
要让它动起来，我们首先需要一些运算！
下一节，我们就来做最简单的算数运算。